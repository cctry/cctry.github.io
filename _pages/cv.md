---
layout: cv-layout
title: "CV"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

## Contact Information
* Email: shiyang.chen@rutgers.edu
* Phone: 978-856-9583
* Location: New Brunswick, NJ 08854
* Website: [GitHub Profile](https://github.com/cctry)

## Summary
Ph.D. Student at Rutgers, The State University of New Jersey, focusing on High-Performance Computing

## Education
* Ph.D. in High-Performance Computing, Rutgers, The State University of New Jersey (2023-Present)
  * Department of Electrical & Computer Engineering
  * Advisor: Hang Liu

* M.S. in High-Performance Computing, Stevens Institute of Technology (2020-2022)
  * Department of Electrical & Computer Engineering
  * Advisor: Hang Liu

* B.E. in Electric Science and Technology, Huazhong University of Science & Technology (2015-2019)
  * School of Optical and Electronic Information

## Work Experience

* **Graduate Research Assistant** at Rutgers, The State University of New Jersey (2023-Present)
  * Research in High-Performance Computing
  * Advisor: Hang Liu

* **Software Engineer Intern** at ByteDance (Summer 2024)
  * Developed a PoC system based on vLLM to disaggregate prefill and decode stages in LLM inference
  * Achieved near-linear latency reduction across multiple nodes
  * Mentor: Wei Xu

* **External Collaborator** at Microsoft Deepspeed (2023-2024)
  * Extended DeepSpeed framework for AI-for-science workloads
  * Developed GPU kernels for model quantization
  * Mentor: Shuaiwen Leon Song

* **Applied Scientist Intern** at Amazon Web Service (Summer 2023)
  * Developed large-scale GNN inference system
  * Reduced processing time from days to under an hour
  * Mentor: Xiang Song

* **Applied Scientist Intern** at Amazon Web Service (2022-06 to 2022-12)
  * Built system for low-latency, staleness-bounded temporal GNN inference on distributed graphs
  * Mentor: Da Zheng

* **Research Intern** at Lawrence Livermore National Laboratory (Summer 2020)
  * Developed GPU-accelerated Monte Carlo sampling framework using ray tracing cores
  * Mentor: Chunhua Liao

## Publications

### 2024
* "FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design" - USENIX Annual Technical Conference (ATC)
* "Kernel fusion in atomistic spin dynamics simulations on Nvidia GPUs using tensor core" - Journal of Computational Science
* "TEA+: A Novel Temporal Graph Random Walk Engine With Hybrid Storage Architecture" - ACM Transactions on Architecture and Code Optimization (TACO)

### 2023
* "TANGO: rethinking quantization for graph neural network training on GPUs" - SC
* "PeeK: A Prune-Centric Approach for K Shortest Path Computation" - SC
* "DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies" - NeurIPS AI for Science Workshop
* "Motif-Based Graph Representation Learning with Application to Chemical Molecules" - Informatics Vol. 10. No. 1

### 2022
* "A Length Adaptive Algorithm-Hardware Co-design of Transformer on FPGA Through Sparse Attention and Dynamic Pipelining" - DAC
* "Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm" - ACL

### 2021
* "E.T.: Rethinking Transformer Models on GPUs" - SC
* "Optimizing FPGA-based Accelerator Design for Large-Scale Molecular Similarity Search" - ICCAD
* "HMC-TRAN: A Tensor-core Inspired Hierarchical Model Compression for Transformer-based DNNs on GPU" - GLVLSI
